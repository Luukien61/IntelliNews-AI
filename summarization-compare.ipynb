{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install transformers torch scikit-learn numpy pandas matplotlib seaborn underthesea vncorenlp -q"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, T5ForConditionalGeneration, BartForConditionalGeneration\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from collections import Counter\n",
    "try:\n",
    "    from underthesea import word_tokenize, sent_tokenize\n",
    "    HAS_UNDERTHESEA = True\n",
    "except ImportError:\n",
    "    HAS_UNDERTHESEA = False"
   ],
   "id": "906104094d901cb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ],
   "id": "50e4ef7bce08f1be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize Vietnamese text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text"
   ],
   "id": "20430c8b86c28c15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def simple_sentence_tokenize(text):\n",
    "    \"\"\"Simple regex-based sentence tokenizer for Vietnamese text.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]"
   ],
   "id": "a7c85934a6667b09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_sentence_tokenizer():\n",
    "    \"\"\"Get the best available sentence tokenizer\"\"\"\n",
    "    if HAS_UNDERTHESEA:\n",
    "        return sent_tokenize\n",
    "    else:\n",
    "        return simple_sentence_tokenize"
   ],
   "id": "8da9b3ad76179193"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sentence_tokenize = get_sentence_tokenizer()",
   "id": "18b4a820bfc8d1e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BaseSummarizer:\n",
    "    def __init__(self, name=\"Base\"):\n",
    "        self.name = name\n",
    "\n",
    "    def summarize(self, text, ratio=0.3):\n",
    "        raise NotImplementedError(\"Each summarizer must implement this method\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name"
   ],
   "id": "dea71b263a76f2fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PhoBERTSummarizer(BaseSummarizer):\n",
    "    def __init__(self, device=None):\n",
    "        super().__init__(name=\"PhoBERT (VietAI)\")\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Initializing PhoBERT on {self.device}\")\n",
    "\n",
    "        # Load PhoBERT model\n",
    "        self.model_name = \"vinai/phobert-base\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name).to(self.device)\n",
    "\n",
    "    def get_sentence_embeddings(self, sentences):\n",
    "        embeddings = []\n",
    "        for sentence in sentences:\n",
    "            # PhoBERT uses word-level tokens\n",
    "            inputs = self.tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "            # Use the [CLS] token embedding as the sentence embedding\n",
    "            sentence_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(sentence_embedding[0])\n",
    "\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def summarize(self, text, ratio=0.3):\n",
    "        # Clean and get sentences\n",
    "        text = clean_text(text)\n",
    "        sentences = sentence_tokenize(text)\n",
    "\n",
    "        if len(sentences) <= 2:\n",
    "            return text, [], np.array([[1]])\n",
    "\n",
    "        # Get sentence embeddings\n",
    "        embeddings = self.get_sentence_embeddings(sentences)\n",
    "\n",
    "        # Compute cosine similarity between sentences\n",
    "        sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "        # Score sentences using the PageRank-like algorithm\n",
    "        scores = np.sum(sim_matrix, axis=1)\n",
    "        ranked_sentences = sorted(((scores[i], i, s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "        # Select top sentences\n",
    "        num_sentences = max(1, int(len(sentences) * ratio))\n",
    "        selected_indices = sorted([item[1] for item in ranked_sentences[:num_sentences]])\n",
    "\n",
    "        # Reconstruct the summary\n",
    "        summary = \" \".join([sentences[i] for i in selected_indices])\n",
    "\n",
    "        return summary, ranked_sentences, sim_matrix"
   ],
   "id": "fab1a71909949b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ViT5Summarizer(BaseSummarizer):\n",
    "    def __init__(self, device=None):\n",
    "        super().__init__(name=\"ViT5\")\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Initializing ViT5 on {self.device}\")\n",
    "\n",
    "        # Load ViT5 model with correct configuration\n",
    "        self.model_name = \"VietAI/vit5-base-vietnews-summarization\"  # Use a model fine-tuned specifically for Vietnamese summarization\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(self.model_name).to(self.device)\n",
    "            print(\"Successfully loaded ViT5 model\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ViT5 model: {e}\")\n",
    "            # Fallback to base model if specific one isn't available\n",
    "            self.model_name = \"VietAI/vit5-base\"\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(self.model_name).to(self.device)\n",
    "            print(\"Using fallback ViT5 base model\")\n",
    "\n",
    "    def summarize(self, text, ratio=0.3):\n",
    "        # Clean text and handle encoding\n",
    "        text = clean_text(text)\n",
    "\n",
    "        # Preparing prompt with specific format for T5\n",
    "        # The actual prompt format depends on how the model was fine-tuned\n",
    "        input_text = f\"summarize: {text}\"  # Many T5 models expect \"summarize: \" prefix\n",
    "\n",
    "        # Different prompt formats to try if first one fails\n",
    "        prompt_formats = [\n",
    "            f\"summarize: {text}\",\n",
    "            f\"tóm tắt: {text}\",\n",
    "            text  # Sometimes no prefix works better\n",
    "        ]\n",
    "\n",
    "        # Try each prompt format until one works\n",
    "        for prompt in prompt_formats:\n",
    "            try:\n",
    "                # Tokenize with careful handling of Vietnamese characters\n",
    "                inputs = self.tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=1024,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    add_special_tokens=True\n",
    "                ).to(self.device)\n",
    "\n",
    "                # Generate summary with careful parameter tuning\n",
    "                with torch.no_grad():\n",
    "                    summary_ids = self.model.generate(\n",
    "                        inputs.input_ids,\n",
    "                        attention_mask=inputs.attention_mask,\n",
    "                        max_length=256,  # Shorter max length to avoid repetition\n",
    "                        min_length=30,   # Ensure a reasonable summary length\n",
    "                        num_beams=4,     # Beam search for better quality\n",
    "                        length_penalty=2.0,  # Encourage longer summaries\n",
    "                        early_stopping=True,\n",
    "                        no_repeat_ngram_size=3,  # Avoid repeated phrases\n",
    "                        bad_words_ids=None,      # Don't explicitly ban any tokens\n",
    "                        do_sample=False          # Deterministic generation\n",
    "                    )\n",
    "\n",
    "                # Decode summary with special handling for Vietnamese\n",
    "                summary = self.tokenizer.decode(\n",
    "                    summary_ids[0],\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=True\n",
    "                )\n",
    "\n",
    "                # Check if summary looks valid (containing actual Vietnamese text)\n",
    "                if any(char in summary for char in \"abcdefghijklmnopqrstuvwxyzáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữựýỳỷỹỵđ\"):\n",
    "                    break  # If valid, use this summary\n",
    "            except Exception as e:\n",
    "                print(f\"Error with prompt format '{prompt[:20]}...': {e}\")\n",
    "                summary = \"Không thể tạo tóm tắt (Unable to generate summary)\"\n",
    "\n",
    "        # For compatibility with visualization functions\n",
    "        sentences = sentence_tokenize(text)\n",
    "        dummy_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "        dummy_ranked = [(1.0, i, s) for i, s in enumerate(sentences)]\n",
    "\n",
    "        # Perform basic post-processing to clean up summary\n",
    "        summary = re.sub(r'[^\\x00-\\x7F\\u00C0-\\u1EF9\\s.,!?:;]', '', summary)  # Remove invalid characters\n",
    "        summary = re.sub(r'\\s+', ' ', summary).strip()  # Fix spacing\n",
    "        summary = re.sub(r'(.+?)\\1{2,}', r'\\1', summary)  # Remove excessive repetition\n",
    "\n",
    "        return summary, dummy_ranked, dummy_matrix"
   ],
   "id": "725ac7b08ecdcb36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TfidfSummarizer(BaseSummarizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"TF-IDF\")\n",
    "\n",
    "    def summarize(self, text, ratio=0.3):\n",
    "        # Clean and get sentences\n",
    "        text = clean_text(text)\n",
    "        sentences = sentence_tokenize(text)\n",
    "\n",
    "        if len(sentences) <= 2:\n",
    "            return text, [], np.array([[1]])\n",
    "\n",
    "        # Compute sentence vectors using TF-IDF\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        try:\n",
    "            sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "        except ValueError:  # Handle case with empty sentences\n",
    "            dummy_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "            dummy_ranked = [(1.0, i, s) for i, s in enumerate(sentences)]\n",
    "            return \" \".join(sentences[:max(1, int(len(sentences) * ratio))]), dummy_ranked, dummy_matrix\n",
    "\n",
    "        # Compute sentence similarity matrix\n",
    "        sim_matrix = cosine_similarity(sentence_vectors)\n",
    "\n",
    "        # Score sentences based on similarity\n",
    "        scores = np.sum(sim_matrix, axis=1)\n",
    "        ranked_sentences = sorted(((scores[i], i, s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "        # Select top sentences\n",
    "        num_sentences = max(1, int(len(sentences) * ratio))\n",
    "        selected_indices = sorted([item[1] for item in ranked_sentences[:num_sentences]])\n",
    "\n",
    "        # Reconstruct the summary\n",
    "        summary = \" \".join([sentences[i] for i in selected_indices])\n",
    "\n",
    "        return summary, ranked_sentences, sim_matrix"
   ],
   "id": "bf1d2427dcebe570"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MultiBERTSummarizer(BaseSummarizer):\n",
    "    def __init__(self, device=None):\n",
    "        super().__init__(name=\"mBERT\")\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Initializing mBERT on {self.device}\")\n",
    "\n",
    "        # Load multilingual BERT model\n",
    "        self.model_name = \"bert-base-multilingual-cased\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name).to(self.device)\n",
    "\n",
    "    def get_sentence_embeddings(self, sentences):\n",
    "        embeddings = []\n",
    "        for sentence in sentences:\n",
    "            inputs = self.tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "            # Use the [CLS] token embedding as the sentence embedding\n",
    "            sentence_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(sentence_embedding[0])\n",
    "\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def summarize(self, text, ratio=0.3):\n",
    "        # Clean and get sentences\n",
    "        text = clean_text(text)\n",
    "        sentences = sentence_tokenize(text)\n",
    "\n",
    "        if len(sentences) <= 2:\n",
    "            return text, [], np.array([[1]])\n",
    "\n",
    "        # Get sentence embeddings\n",
    "        embeddings = self.get_sentence_embeddings(sentences)\n",
    "\n",
    "        # Compute cosine similarity between sentences\n",
    "        sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "        # Score sentences using the PageRank-like algorithm\n",
    "        scores = np.sum(sim_matrix, axis=1)\n",
    "        ranked_sentences = sorted(((scores[i], i, s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "        # Select top sentences\n",
    "        num_sentences = max(1, int(len(sentences) * ratio))\n",
    "        selected_indices = sorted([item[1] for item in ranked_sentences[:num_sentences]])\n",
    "\n",
    "        # Reconstruct the summary\n",
    "        summary = \" \".join([sentences[i] for i in selected_indices])\n",
    "\n",
    "        return summary, ranked_sentences, sim_matrix"
   ],
   "id": "bfc79429403cfe4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PositionSummarizer(BaseSummarizer):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"Position-based\")\n",
    "\n",
    "    def summarize(self, text, ratio=0.3):\n",
    "        # Clean and get sentences\n",
    "        text = clean_text(text)\n",
    "        sentences = sentence_tokenize(text)\n",
    "\n",
    "        if len(sentences) <= 2:\n",
    "            return text, [], np.array([[1]])\n",
    "\n",
    "        # Score sentences based on their position (earlier = more important)\n",
    "        # Give highest weight to first sentence, then gradually decrease\n",
    "        num_sent = len(sentences)\n",
    "        scores = [1.0 - (i/num_sent) for i in range(num_sent)]\n",
    "\n",
    "        # Create ranked sentences\n",
    "        ranked_sentences = sorted(((scores[i], i, s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "        # Select top sentences\n",
    "        num_sentences = max(1, int(len(sentences) * ratio))\n",
    "        selected_indices = sorted([item[1] for item in ranked_sentences[:num_sentences]])\n",
    "\n",
    "        # Reconstruct the summary\n",
    "        summary = \" \".join([sentences[i] for i in selected_indices])\n",
    "\n",
    "        # Create a dummy similarity matrix for visualization\n",
    "        sim_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "        return summary, ranked_sentences, sim_matrix"
   ],
   "id": "690784104a631512"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize summarizers\n",
    "    summarizers = [\n",
    "        TfidfSummarizer(),  # Classical TF-IDF approach\n",
    "        PositionSummarizer(),  # Position-based simple baseline\n",
    "        PhoBERTSummarizer(device),  # VietAI's PhoBERT\n",
    "    ]\n",
    "\n",
    "   # Optionally add more complex models if GPU available\n",
    "    if torch.cuda.is_available():\n",
    "        summarizers.append(MultiBERTSummarizer(device))  # Multilingual BERT\n",
    "        try:\n",
    "            # Uncomment to test ViT5 separately first\n",
    "            test_vit5 = False  # Set to True to test ViT5 in isolation\n",
    "            if test_vit5:\n",
    "                print(\"\\nTesting ViT5 summarizer in isolation...\")\n",
    "                vit5 = ViT5Summarizer(device)\n",
    "                for text_name, text in [(\"Tech Article\", vietnamese_tech_text), (\"Tourism Article\", vietnamese_tourism_text)]:\n",
    "                    print(f\"\\nTesting on {text_name}...\")\n",
    "                    vit5_summary, _, _ = vit5.summarize(text)\n",
    "                    print(f\"ViT5 Summary ({len(vit5_summary)} chars):\")\n",
    "                    print(\"=\"*80)\n",
    "                    print(vit5_summary)\n",
    "                    print(\"=\"*80)\n",
    "            else:\n",
    "                # Add ViT5 to the main comparison\n",
    "                summarizers.append(ViT5Summarizer(device))  # Vietnamese T5\n",
    "        except Exception as e:\n",
    "            print(f\"Note: ViT5 model could not be loaded: {e}\")\n",
    "\n",
    "            # Try MT5 as alternative\n",
    "            try:\n",
    "                print(\"\\nTrying MT5 as alternative to ViT5...\")\n",
    "                from transformers import MT5ForConditionalGeneration\n",
    "\n",
    "                class MT5Summarizer(BaseSummarizer):\n",
    "                    def __init__(self, device=None):\n",
    "                        super().__init__(name=\"MT5\")\n",
    "                        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                        print(f\"Initializing MT5 on {self.device}\")\n",
    "\n",
    "                        # Load MT5 model\n",
    "                        self.model_name = \"google/mt5-small\"\n",
    "                        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "                        self.model = MT5ForConditionalGeneration.from_pretrained(self.model_name).to(self.device)\n",
    "\n",
    "                    def summarize(self, text, ratio=0.3):\n",
    "                        # Clean text\n",
    "                        text = clean_text(text)\n",
    "\n",
    "                        # Prepare with Vietnamese prompt\n",
    "                        input_text = f\"summarize to Vietnamese: {text}\"\n",
    "\n",
    "                        # Tokenize input\n",
    "                        inputs = self.tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(self.device)\n",
    "\n",
    "                        # Generate summary\n",
    "                        with torch.no_grad():\n",
    "                            summary_ids = self.model.generate(\n",
    "                                inputs.input_ids,\n",
    "                                max_length=150,\n",
    "                                min_length=40,\n",
    "                                num_beams=4,\n",
    "                                early_stopping=True\n",
    "                            )\n",
    "\n",
    "                        # Decode summary\n",
    "                        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                        # For compatibility with visualization functions\n",
    "                        sentences = sentence_tokenize(text)\n",
    "                        dummy_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "                        dummy_ranked = [(1.0, i, s) for i, s in enumerate(sentences)]\n",
    "\n",
    "                        return summary, dummy_ranked, dummy_matrix\n",
    "\n",
    "                summarizers.append(MT5Summarizer(device))\n",
    "            except Exception as e:\n",
    "                print(f\"Note: MT5 model could not be loaded either: {e}\")\n",
    "\n",
    "    print(f\"Initialized {len(summarizers)} summarization models\")\n",
    "\n",
    "    # Compare models on tech article\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE 1: VIETNAMESE TECHNOLOGY NEWS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"ORIGINAL TEXT:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(vietnamese_tech_text)\n",
    "    print()\n",
    "    tech_results = compare_summaries(vietnamese_tech_text, summarizers, ratio=0.4)\n",
    "\n",
    "    # Display comparison table\n",
    "    tech_comparison = display_comparison_table(tech_results, len(vietnamese_tech_text))\n",
    "    print(\"\\nModel Comparison for Technology Text:\")\n",
    "    print(tech_comparison)\n",
    "\n",
    "    # Plot performance comparison\n",
    "    plot_performance_comparison(tech_results)\n",
    "\n",
    "    # Display summaries\n",
    "    for model, result in tech_results.items():\n",
    "        print(f\"\\n{model} Summary:\")\n",
    "        print(\"=\"*80)\n",
    "        print(result[\"summary\"])\n",
    "\n",
    "    # Compare models on tourism article\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE 2: VIETNAMESE TOURISM ARTICLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"ORIGINAL TEXT:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(vietnamese_tourism_text)\n",
    "    print()\n",
    "    tourism_results = compare_summaries(vietnamese_tourism_text, summarizers, ratio=0.4)\n",
    "\n",
    "    # Display comparison table\n",
    "    tourism_comparison = display_comparison_table(tourism_results, len(vietnamese_tourism_text))\n",
    "    print(\"\\nModel Comparison for Tourism Text:\")\n",
    "    print(tourism_comparison)\n",
    "\n",
    "    # Plot performance comparison\n",
    "    plot_performance_comparison(tourism_results)\n",
    "\n",
    "    # Display summaries\n",
    "    for model, result in tourism_results.items():\n",
    "        print(f\"\\n{model} Summary:\")\n",
    "        print(\"=\"*80)\n",
    "        print(result[\"summary\"])\n",
    "\n",
    "    # Visualize PhoBERT results in detail (as it's our primary model)\n",
    "    if \"PhoBERT (VietAI)\" in tech_results:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DETAILED ANALYSIS OF PHOBERT MODEL\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Tech text visualization\n",
    "        phobert_tech = tech_results[\"PhoBERT (VietAI)\"]\n",
    "        plot_similarity_matrix(phobert_tech[\"sim_matrix\"],\n",
    "                              sentence_tokenize(clean_text(vietnamese_tech_text)),\n",
    "                              \"PhoBERT: Tech Article Similarity Matrix\")\n",
    "\n",
    "        plot_sentence_scores(phobert_tech[\"ranked_sentences\"],\n",
    "                            sentence_tokenize(clean_text(vietnamese_tech_text)),\n",
    "                            \"PhoBERT: Tech Article Sentence Scores\")\n",
    "\n",
    "        # Tourism text visualization\n",
    "        phobert_tourism = tourism_results[\"PhoBERT (VietAI)\"]\n",
    "        plot_similarity_matrix(phobert_tourism[\"sim_matrix\"],\n",
    "                              sentence_tokenize(clean_text(vietnamese_tourism_text)),\n",
    "                              \"PhoBERT: Tourism Article Similarity Matrix\")\n",
    "\n",
    "        plot_sentence_scores(phobert_tourism[\"ranked_sentences\"],\n",
    "                            sentence_tokenize(clean_text(vietnamese_tourism_text)),\n",
    "                            \"PhoBERT: Tourism Article Sentence Scores\")"
   ],
   "id": "fd8bf6dd13abc280"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
